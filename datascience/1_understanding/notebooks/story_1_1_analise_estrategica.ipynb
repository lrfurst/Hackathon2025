{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf2a8aa",
   "metadata": {},
   "source": [
    "# ü§ñ EPIC 1: COMPREENS√ÉO DO PROBLEMA E DADOS\n",
    "\n",
    "*   üìå STORY 1.1: An√°lise Explorat√≥ria Estrat√©gica\n",
    "*   üìå STORY 1.2: Defini√ß√£o do MVP de Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e3d64",
   "metadata": {},
   "source": [
    "## T1.1.1: üîç Diagn√≥stico ultrarr√°pido do dataset\n",
    "\n",
    "Refatorar a Story 1.1 para focar nas 5-7 ou 10 features mais impactantes, seguindo as tasks especificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfef7cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Story 1.1: An√°lise Explorat√≥ria Estrat√©gica\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "# üìä STORY 1.1: AN√ÅLISE EXPLORAT√ìRIA ESTRAT√âGICA\n",
    "## üìã Vis√£o Geral\n",
    "Esta Story implementa uma an√°lise explorat√≥ria estrat√©gica do dataset de voos, focando em:\n",
    "1. Diagn√≥stico ultrarr√°pido do dataset\n",
    "2. An√°lise da vari√°vel alvo\n",
    "3. Identifica√ß√£o de features promissoras\n",
    "4. Detec√ß√£o de problemas cr√≠ticos\n",
    "5. Visualiza√ß√µes estrat√©gicas\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTA√á√ïES NECESS√ÅRIAS\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "# Configura√ß√µes de exibi√ß√£o\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURA√á√ïES INICIAIS\n",
    "# ============================================================================\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_PATH = \"datascience/1_understanding/data/flight_data_2024_sample.csv\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ STORY 1.1: AN√ÅLISE EXPLORAT√ìRIA ESTRAT√âGICA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1.1.1: üîç DIAGN√ìSTICO ULTRARR√ÅPIDO DO DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç TASK 1.1.1: DIAGN√ìSTICO ULTRARR√ÅPIDO DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Carregar dataset otimizado\n",
    "print(\"\\nüì• 1. CARREGAMENTO DO DATASET (OTIMIZADO)\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Mapear tipos de dados para otimiza√ß√£o\n",
    "dtype_mapping = {\n",
    "    'int64': 'int32',\n",
    "    'float64': 'float32'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Primeiro carrega para identificar tipos\n",
    "    df_sample = pd.read_csv(DATA_PATH, nrows=1000)\n",
    "    \n",
    "    # Identificar colunas num√©ricas para otimiza√ß√£o\n",
    "    optimize_cols = {}\n",
    "    for col in df_sample.columns:\n",
    "        if df_sample[col].dtype == 'int64':\n",
    "            optimize_cols[col] = 'int32'\n",
    "        elif df_sample[col].dtype == 'float64':\n",
    "            optimize_cols[col] = 'float32'\n",
    "    \n",
    "    # Carregar dataset completo com tipos otimizados\n",
    "    df = pd.read_csv(DATA_PATH, dtype=optimize_cols, low_memory=True)\n",
    "    \n",
    "    elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "    print(f\"‚úÖ Dataset carregado em {elapsed_time:.1f} segundos\")\n",
    "    print(f\"   ‚Ä¢ Shape: {df.shape[0]:,} linhas √ó {df.shape[1]} colunas\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Arquivo n√£o encontrado: {DATA_PATH}\")\n",
    "    print(\"üìÅ Arquivos no diret√≥rio atual:\")\n",
    "    for f in os.listdir('.'):\n",
    "        if f.endswith('.csv'):\n",
    "            print(f\"  ‚Ä¢ {f}\")\n",
    "    raise SystemExit(\"N√£o √© poss√≠vel prosseguir\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar dataset: {e}\")\n",
    "    raise SystemExit(\"N√£o √© poss√≠vel prosseguir\")\n",
    "\n",
    "# 2. Verificar shape e tipos b√°sicos\n",
    "print(\"\\nüìä 2. INFORMA√á√ïES B√ÅSICAS DO DATASET\")\n",
    "print(f\"Nomes das colunas ({len(df.columns)}):\")\n",
    "cols_per_line = 5\n",
    "for i in range(0, len(df.columns), cols_per_line):\n",
    "    print(\"  \" + \", \".join(df.columns[i:i+cols_per_line]))\n",
    "\n",
    "print(f\"\\nüìã Tipos de dados:\")\n",
    "type_counts = df.dtypes.value_counts()\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"  ‚Ä¢ {dtype}: {count} colunas\")\n",
    "\n",
    "# 3. Identificar colunas de atraso\n",
    "print(\"\\n‚è±Ô∏è 3. IDENTIFICA√á√ÉO DE COLUNAS DE ATRASO\")\n",
    "delay_columns = [col for col in df.columns if 'delay' in col.lower() or 'atraso' in col.lower()]\n",
    "print(f\"Colunas relacionadas a atraso encontradas ({len(delay_columns)}):\")\n",
    "for col in delay_columns:\n",
    "    dtype = df[col].dtype\n",
    "    unique_vals = df[col].nunique()\n",
    "    null_pct = (df[col].isnull().sum() / len(df) * 100)\n",
    "    print(f\"  ‚Ä¢ {col}: {dtype}, {unique_vals} valores √∫nicos, {null_pct:.1f}% nulos\")\n",
    "\n",
    "# 4. Verificar missing values b√°sicos\n",
    "print(\"\\n‚ùì 4. AN√ÅLISE DE VALORES AUSENTES\")\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Tipo': df.dtypes,\n",
    "    'Valores √önicos': df.nunique(),\n",
    "    'Valores Nulos': df.isnull().sum(),\n",
    "    '% Nulos': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"\\nüìä RESUMO DE VALORES AUSENTES:\")\n",
    "print(f\"Total de c√©lulas: {df.shape[0] * df.shape[1]:,}\")\n",
    "print(f\"Valores nulos totais: {df.isnull().sum().sum():,}\")\n",
    "print(f\"Porcentagem geral de nulos: {(df.isnull().sum().sum() / (df.shape[0] * df.shape[1]) * 100):.2f}%\")\n",
    "\n",
    "print(\"\\nüîù TOP 10 COLUNAS COM MAIS VALORES AUSENTES:\")\n",
    "top_missing = missing_summary.sort_values('% Nulos', ascending=False).head(10)\n",
    "display(top_missing)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1.1.2: üìà AN√ÅLISE DE DISTRIBUI√á√ÉO DA VARI√ÅVEL ALVO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìà TASK 1.1.2: AN√ÅLISE DA VARI√ÅVEL ALVO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Criar vari√°vel alvo bin√°ria\n",
    "print(\"\\nüéØ 1. CRIANDO VARI√ÅVEL ALVO BIN√ÅRIA\")\n",
    "LIMITE_ATRASO = 15\n",
    "\n",
    "# Identificar a melhor coluna de atraso para usar como alvo\n",
    "target_col = None\n",
    "preferred_cols = ['arr_delay', 'ARR_DELAY', 'ArrivalDelay', 'atraso_chegada']\n",
    "\n",
    "for col in preferred_cols:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None and len(delay_columns) > 0:\n",
    "    target_col = delay_columns[0]\n",
    "\n",
    "if target_col:\n",
    "    print(f\"‚úÖ Usando coluna '{target_col}' para criar vari√°vel alvo\")\n",
    "    \n",
    "    # Criar vari√°vel alvo bin√°ria\n",
    "    df['target_atraso'] = df[target_col].fillna(0).apply(\n",
    "        lambda x: 1 if x >= LIMITE_ATRASO else 0\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Vari√°vel 'target_atraso' criada (atraso ‚â• {LIMITE_ATRASO} min)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Nenhuma coluna de atraso identificada claramente\")\n",
    "    print(\"üîç Procurando por colunas num√©ricas que possam representar atraso...\")\n",
    "    \n",
    "    # Procurar por colunas num√©ricas com valores que fazem sentido para atraso\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if len(unique_vals) > 0:\n",
    "            min_val, max_val = unique_vals.min(), unique_vals.max()\n",
    "            if -60 <= min_val <= 300 and -60 <= max_val <= 300:  # Faixa razo√°vel para atraso\n",
    "                print(f\"  ‚Ä¢ Poss√≠vel coluna de atraso: {col} (valores entre {min_val} e {max_val})\")\n",
    "                target_col = col\n",
    "                break\n",
    "    \n",
    "    if target_col:\n",
    "        df['target_atraso'] = df[target_col].fillna(0).apply(\n",
    "            lambda x: 1 if x >= LIMITE_ATRASO else 0\n",
    "        )\n",
    "        print(f\"‚úÖ Vari√°vel 'target_atraso' criada a partir de '{target_col}'\")\n",
    "    else:\n",
    "        print(\"‚ùå N√£o foi poss√≠vel identificar coluna para vari√°vel alvo\")\n",
    "        raise SystemExit(\"N√£o √© poss√≠vel prosseguir sem vari√°vel alvo\")\n",
    "\n",
    "# 2. Calcular taxa base de atrasos\n",
    "print(\"\\nüìä 2. TAXA BASE DE ATRASOS\")\n",
    "target_stats = df['target_atraso'].value_counts().sort_index()\n",
    "total_flights = len(df)\n",
    "\n",
    "print(f\"Total de voos analisados: {total_flights:,}\")\n",
    "print(f\"\\nDistribui√ß√£o da vari√°vel alvo:\")\n",
    "for value, count in target_stats.items():\n",
    "    percentage = (count / total_flights) * 100\n",
    "    label = \"ATRASADO (‚â•15min)\" if value == 1 else \"PONTUAL (<15min)\"\n",
    "    print(f\"  ‚Ä¢ {label}: {count:,} voos ({percentage:.2f}%)\")\n",
    "\n",
    "taxa_atraso = target_stats.get(1, 0) / total_flights * 100\n",
    "print(f\"\\nüéØ Taxa base de atrasos: {taxa_atraso:.2f}%\")\n",
    "\n",
    "# 3. Verificar balanceamento\n",
    "print(\"\\n‚öñÔ∏è 3. VERIFICA√á√ÉO DE BALANCEAMENTO\")\n",
    "minority_class = min(target_stats.values)\n",
    "majority_class = max(target_stats.values)\n",
    "balance_ratio = minority_class / majority_class\n",
    "\n",
    "print(f\"Classe minorit√°ria: {minority_class:,} voos\")\n",
    "print(f\"Classe majorit√°ria: {majority_class:,} voos\")\n",
    "print(f\"Raz√£o de balanceamento: {balance_ratio:.3f}\")\n",
    "\n",
    "if balance_ratio < 0.5:\n",
    "    print(\"‚ö†Ô∏è  Dataset DESBALANCEADO - necessidade de t√©cnicas especiais\")\n",
    "    if balance_ratio < 0.3:\n",
    "        print(\"üî¥ FORTEMENTE DESBALANCEADO - aten√ß√£o especial necess√°ria\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset razoavelmente balanceado\")\n",
    "\n",
    "# 4. Documentar baseline para m√©tricas\n",
    "print(\"\\nüìà 4. BASELINE PARA M√âTRICAS\")\n",
    "# Baseline: prever sempre a classe majorit√°ria\n",
    "majority_class_label = 0 if target_stats.get(0, 0) > target_stats.get(1, 0) else 1\n",
    "baseline_accuracy = target_stats.get(majority_class_label, 0) / total_flights * 100\n",
    "\n",
    "print(f\"Classe majorit√°ria: {'Pontual (0)' if majority_class_label == 0 else 'Atrasado (1)'}\")\n",
    "print(f\"Acur√°cia do baseline (prever sempre majorit√°ria): {baseline_accuracy:.2f}%\")\n",
    "print(f\"Recall do baseline para atrasos: {100 if majority_class_label == 1 else 0:.2f}%\")\n",
    "print(f\"Precision do baseline para atrasos: {taxa_atraso if majority_class_label == 1 else 0:.2f}%\")\n",
    "\n",
    "# Salvar an√°lise da vari√°vel alvo\n",
    "print(\"\\nüíæ SALVANDO AN√ÅLISE DA VARI√ÅVEL ALVO...\")\n",
    "target_analysis = pd.DataFrame({\n",
    "    'total_flights': [total_flights],\n",
    "    'delayed_flights': [target_stats.get(1, 0)],\n",
    "    'on_time_flights': [target_stats.get(0, 0)],\n",
    "    'delay_rate_pct': [taxa_atraso],\n",
    "    'balance_ratio': [balance_ratio],\n",
    "    'baseline_accuracy': [baseline_accuracy],\n",
    "    'target_column': [target_col],\n",
    "    'threshold_minutes': [LIMITE_ATRASO]\n",
    "})\n",
    "\n",
    "# Criar diret√≥rios se n√£o existirem\n",
    "os.makedirs('datascience/1_understanding/data', exist_ok=True)\n",
    "os.makedirs('datascience/1_understanding/docs', exist_ok=True)\n",
    "\n",
    "target_analysis.to_csv('datascience/1_understanding/data/target_variable_analysis.csv', index=False)\n",
    "print(\"‚úÖ An√°lise salva em 'datascience/1_understanding/data/target_variable_analysis.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1.1.3: üéØ IDENTIFICA√á√ÉO DE FEATURES PROMISSORAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ TASK 1.1.3: IDENTIFICA√á√ÉO DE FEATURES PROMISSORAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Correla√ß√£o r√°pida com vari√°vel alvo\n",
    "print(\"\\nüîó 1. CORRELA√á√ÉO COM VARI√ÅVEL ALVO\")\n",
    "\n",
    "# Selecionar apenas colunas num√©ricas para correla√ß√£o\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remover a pr√≥pria vari√°vel alvo se estiver na lista\n",
    "if 'target_atraso' in numeric_cols:\n",
    "    numeric_cols.remove('target_atraso')\n",
    "\n",
    "# Remover colunas que n√£o fazem sentido para correla√ß√£o (IDs, c√≥digos)\n",
    "exclude_patterns = ['id', 'ID', 'code', 'CODE', 'num', 'NUM', 'flt', 'FLT']\n",
    "filtered_numeric_cols = []\n",
    "for col in numeric_cols:\n",
    "    if not any(pattern in col.upper() for pattern in [p.upper() for p in exclude_patterns]):\n",
    "        filtered_numeric_cols.append(col)\n",
    "\n",
    "# Calcular correla√ß√£o apenas com as colunas filtradas\n",
    "if len(filtered_numeric_cols) > 0:\n",
    "    correlation_with_target = {}\n",
    "    \n",
    "    for col in filtered_numeric_cols[:50]:  # Limitar para performance\n",
    "        try:\n",
    "            # Remover valores nulos para c√°lculo de correla√ß√£o\n",
    "            valid_data = df[[col, 'target_atraso']].dropna()\n",
    "            if len(valid_data) > 100:  # Apenas se tiver dados suficientes\n",
    "                corr = valid_data[col].corr(valid_data['target_atraso'])\n",
    "                if not pd.isna(corr):  # Verificar se n√£o √© NaN\n",
    "                    correlation_with_target[col] = corr\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Ordenar por valor absoluto da correla√ß√£o\n",
    "    sorted_corr = sorted(correlation_with_target.items(), \n",
    "                        key=lambda x: abs(x[1]), \n",
    "                        reverse=True)\n",
    "    \n",
    "    print(f\"\\nüîù TOP 10 FEATURES POR CORRELA√á√ÉO (absoluta) COM ATRASO:\")\n",
    "    for i, (col, corr) in enumerate(sorted_corr[:10]):\n",
    "        direction = \"positiva\" if corr > 0 else \"negativa\"\n",
    "        print(f\"  {i+1:2d}. {col:30} | r = {corr:7.3f} ({direction})\")\n",
    "    \n",
    "    # Salvar top 10 features\n",
    "    top_features = [col for col, _ in sorted_corr[:10]]\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  N√£o h√° colunas num√©ricas suficientes para an√°lise de correla√ß√£o\")\n",
    "    top_features = []\n",
    "    sorted_corr = []\n",
    "\n",
    "# 2. Verificar colunas temporais\n",
    "print(\"\\n‚è∞ 2. COLUNAS TEMPORAIS IDENTIFICADAS\")\n",
    "time_patterns = ['date', 'DATE', 'time', 'TIME', 'datetime', 'DATETIME', \n",
    "                 'hora', 'HORA', 'dt', 'DT', 'timestamp', 'TIMESTAMP']\n",
    "\n",
    "time_cols = []\n",
    "for col in df.columns:\n",
    "    if any(pattern in col for pattern in time_patterns):\n",
    "        time_cols.append(col)\n",
    "\n",
    "print(f\"Colunas temporais encontradas ({len(time_cols)}):\")\n",
    "for col in time_cols:\n",
    "    dtype = str(df[col].dtype)\n",
    "    unique_vals = df[col].nunique()\n",
    "    sample_val = str(df[col].iloc[0])[:50] if len(df) > 0 else \"N/A\"\n",
    "    print(f\"  ‚Ä¢ {col:25} | {dtype:10} | {unique_vals:5} √∫nicos | Ex: {sample_val}\")\n",
    "\n",
    "# 3. Identificar colunas categ√≥ricas chave\n",
    "print(\"\\nüè∑Ô∏è 3. COLUNAS CATEG√ìRICAS CHAVE\")\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Filtrar para colunas com significado potencial\n",
    "meaningful_categorical = []\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    # Considerar como chave se tiver entre 2 e 100 valores √∫nicos\n",
    "    if 2 <= unique_count <= 100:\n",
    "        meaningful_categorical.append((col, unique_count))\n",
    "\n",
    "print(f\"Colunas categ√≥ricas significativas ({len(meaningful_categorical)}):\")\n",
    "meaningful_categorical.sort(key=lambda x: x[1])\n",
    "\n",
    "for col, count in meaningful_categorical[:15]:  # Mostrar apenas as primeiras\n",
    "    print(f\"  ‚Ä¢ {col:25} | {count:3} valores √∫nicos\")\n",
    "\n",
    "# 4. Criar lista de 10 features potenciais\n",
    "print(\"\\nüìã 4. LISTA DE 10 FEATURES POTENCIAIS\")\n",
    "\n",
    "# Combinar features baseadas em diferentes crit√©rios\n",
    "potential_features = []\n",
    "\n",
    "# Adicionar top correlacionadas\n",
    "if sorted_corr:\n",
    "    potential_features.extend([col for col, _ in sorted_corr[:5]])\n",
    "\n",
    "# Adicionar categ√≥ricas mais promissoras\n",
    "for col, count in meaningful_categorical[:3]:\n",
    "    if col not in potential_features:\n",
    "        potential_features.append(col)\n",
    "\n",
    "# Adicionar temporais mais importantes\n",
    "for col in time_cols[:2]:\n",
    "    if col not in potential_features:\n",
    "        potential_features.append(col)\n",
    "\n",
    "# Completar com outras features baseadas em nome\n",
    "feature_keywords = ['airline', 'carrier', 'origin', 'dest', 'dep', 'arr', \n",
    "                    'delay', 'time', 'dist', 'weather', 'taxi']\n",
    "\n",
    "for keyword in feature_keywords:\n",
    "    for col in df.columns:\n",
    "        if keyword.lower() in col.lower() and col not in potential_features:\n",
    "            if len(potential_features) < 10:\n",
    "                potential_features.append(col)\n",
    "            else:\n",
    "                break\n",
    "    if len(potential_features) >= 10:\n",
    "        break\n",
    "\n",
    "print(\"\\nüéØ LISTA FINAL DE 10 FEATURES POTENCIAIS:\")\n",
    "for i, feature in enumerate(potential_features[:10], 1):\n",
    "    feature_type = \"num√©rico\" if feature in numeric_cols else \"categ√≥rico\" if feature in categorical_cols else \"temporal\"\n",
    "    print(f\"  {i:2d}. {feature:25} | {feature_type:15}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1.1.4: ‚ö†Ô∏è DETEC√á√ÉO DE PROBLEMAS CR√çTICOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚ö†Ô∏è TASK 1.1.4: DETEC√á√ÉO DE PROBLEMAS CR√çTICOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Verificar vazamento de dados\n",
    "print(\"\\nüîí 1. VERIFICA√á√ÉO DE VAZAMENTO DE DADOS\")\n",
    "print(\"Procurando colunas que podem conter informa√ß√£o do futuro...\")\n",
    "\n",
    "leakage_keywords = ['delay', 'atraso', 'arrival', 'chegada', 'actual', 'real']\n",
    "leakage_columns = []\n",
    "\n",
    "for col in df.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(keyword in col_lower for keyword in leakage_keywords):\n",
    "        # Verificar se n√£o √© a coluna alvo que criamos\n",
    "        if col != 'target_atraso' and col != target_col:\n",
    "            leakage_columns.append(col)\n",
    "\n",
    "print(f\"Colunas suspeitas de vazamento encontradas ({len(leakage_columns)}):\")\n",
    "for col in leakage_columns[:10]:  # Mostrar apenas as primeiras\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "if leakage_columns:\n",
    "    print(\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Estas colunas podem causar vazamento de dados!\")\n",
    "    print(\"   Considerar remover durante a modelagem.\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma coluna √≥bvia de vazamento identificada\")\n",
    "\n",
    "# 2. Identificar colunas com >50% missing\n",
    "print(\"\\n‚ùì 2. COLUNAS COM MAIS DE 50% DE VALORES AUSENTES\")\n",
    "high_missing_threshold = 50\n",
    "high_missing_cols = missing_summary[missing_summary['% Nulos'] > high_missing_threshold]\n",
    "\n",
    "print(f\"Colunas com >{high_missing_threshold}% de valores ausentes ({len(high_missing_cols)}):\")\n",
    "if len(high_missing_cols) > 0:\n",
    "    for idx, row in high_missing_cols.iterrows():\n",
    "        print(f\"  ‚Ä¢ {idx:25} | {row['% Nulos']:.1f}% nulos | {row['Tipo']}\")\n",
    "    \n",
    "    print(f\"\\nüö´ RECOMENDA√á√ÉO: Remover estas {len(high_missing_cols)} colunas\")\n",
    "else:\n",
    "    print(\"‚úÖ Nenhuma coluna com mais de 50% de valores ausentes\")\n",
    "\n",
    "# 3. Detectar outliers extremos em vari√°veis num√©ricas\n",
    "print(\"\\nüìä 3. DETEC√á√ÉO DE OUTLIERS EXTREMOS\")\n",
    "if len(filtered_numeric_cols) > 0:\n",
    "    print(\"Analisando outliers nas principais vari√°veis num√©ricas...\")\n",
    "    \n",
    "    outlier_analysis = []\n",
    "    for col in filtered_numeric_cols[:10]:  # Analisar apenas as primeiras 10\n",
    "        data = df[col].dropna()\n",
    "        if len(data) > 0:\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "            outlier_pct = (len(outliers) / len(data)) * 100\n",
    "            \n",
    "            if outlier_pct > 5:  # Mais de 5% de outliers\n",
    "                outlier_analysis.append({\n",
    "                    'coluna': col,\n",
    "                    'outliers_count': len(outliers),\n",
    "                    'outliers_pct': outlier_pct,\n",
    "                    'min': data.min(),\n",
    "                    'max': data.max()\n",
    "                })\n",
    "    \n",
    "    if outlier_analysis:\n",
    "        print(f\"Vari√°veis com mais de 5% de outliers extremos ({len(outlier_analysis)}):\")\n",
    "        for analysis in outlier_analysis:\n",
    "            print(f\"  ‚Ä¢ {analysis['coluna']:25} | {analysis['outliers_pct']:.1f}% outliers \"\n",
    "                  f\"({analysis['outliers_count']:,} de {len(df[analysis['coluna']].dropna()):,})\")\n",
    "    else:\n",
    "        print(\"‚úÖ Nenhuma vari√°vel com mais de 5% de outliers extremos\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  N√£o h√° vari√°veis num√©ricas para an√°lise de outliers\")\n",
    "    outlier_analysis = []\n",
    "\n",
    "# 4. Documentar limita√ß√µes do dataset\n",
    "print(\"\\nüìù 4. LIMITA√á√ïES DOCUMENTADAS DO DATASET\")\n",
    "\n",
    "limitations = []\n",
    "\n",
    "# Limita√ß√µes baseadas na an√°lise\n",
    "if len(high_missing_cols) > 0:\n",
    "    limitations.append(f\"‚Ä¢ {len(high_missing_cols)} colunas com >50% de valores ausentes\")\n",
    "\n",
    "if leakage_columns:\n",
    "    limitations.append(f\"‚Ä¢ {len(leakage_columns)} colunas suspeitas de vazamento de dados\")\n",
    "\n",
    "if balance_ratio < 0.5:\n",
    "    limitations.append(f\"‚Ä¢ Dataset desbalanceado (raz√£o: {balance_ratio:.3f})\")\n",
    "\n",
    "if len(numeric_cols) < 5:\n",
    "    limitations.append(\"‚Ä¢ Poucas vari√°veis num√©ricas dispon√≠veis\")\n",
    "\n",
    "if len(time_cols) == 0:\n",
    "    limitations.append(\"‚Ä¢ Sem colunas temporais expl√≠citas\")\n",
    "\n",
    "print(\"\\nüö´ PRINCIPAIS LIMITA√á√ïES IDENTIFICADAS:\")\n",
    "for i, limitation in enumerate(limitations, 1):\n",
    "    print(f\"  {i}. {limitation}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1.1.5: üìä VISUALIZA√á√ïES ESTRAT√âGICAS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä TASK 1.1.5: VISUALIZA√á√ïES ESTRAT√âGICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüé® GERANDO VISUALIZA√á√ïES ESTRAT√âGICAS...\")\n",
    "\n",
    "# Criar figura com subplots\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "fig.suptitle('DASHBOARD ESTRAT√âGICO - AN√ÅLISE EXPLORAT√ìRIA DO DATASET DE VOOS',\n",
    "             fontsize=18, fontweight='bold', y=1.02)\n",
    "\n",
    "# 1. Distribui√ß√£o da vari√°vel alvo\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "contagem_alvo = df['target_atraso'].value_counts().sort_index()\n",
    "cores_alvo = ['#2ecc71', '#e74c3c']\n",
    "labels_alvo = [f'Pontual\\n({contagem_alvo.get(0, 0):,})', \n",
    "               f'Atrasado\\n({contagem_alvo.get(1, 0):,})']\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(contagem_alvo, labels=labels_alvo, \n",
    "                                   autopct='%1.1f%%', colors=cores_alvo, \n",
    "                                   explode=(0, 0.05), startangle=90,\n",
    "                                   textprops={'fontsize': 10})\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax1.set_title('Distribui√ß√£o da Vari√°vel Alvo', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Heatmap de correla√ß√£o (top 15 features) - VERS√ÉO CORRIGIDA\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.set_title('Heatmap de Correla√ß√£o', fontsize=14, fontweight='bold')\n",
    "\n",
    "if len(sorted_corr) >= 5:\n",
    "    # Selecionar top 15 features para correla√ß√£o\n",
    "    top_corr_features = [col for col, _ in sorted_corr[:15]]\n",
    "    \n",
    "    # Adicionar vari√°vel alvo\n",
    "    if 'target_atraso' not in top_corr_features:\n",
    "        top_corr_features.append('target_atraso')\n",
    "    \n",
    "    # VERIFICA√á√ÉO DE SEGURAN√áA ANTES DE CALCULAR CORRELA√á√ÉO\n",
    "    safe_features = []\n",
    "    problematic_features = []\n",
    "    \n",
    "    print(\"\\nüîç VERIFICANDO FEATURES PARA CORRELA√á√ÉO SEGURA:\")\n",
    "    \n",
    "    for col in top_corr_features:\n",
    "        if col in df.columns:\n",
    "            # Verificar se √© coluna num√©rica\n",
    "            if df[col].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                # Verificar problemas conhecidos\n",
    "                non_null_data = df[col].dropna()\n",
    "                \n",
    "                if len(non_null_data) < 10:\n",
    "                    print(f\"  ‚ö†Ô∏è  {col}: Muito poucos dados ({len(non_null_data)} valores n√£o nulos)\")\n",
    "                    problematic_features.append(col)\n",
    "                elif non_null_data.nunique() == 1:\n",
    "                    print(f\"  ‚ö†Ô∏è  {col}: Todos valores iguais (desvio padr√£o zero)\")\n",
    "                    problematic_features.append(col)\n",
    "                else:\n",
    "                    # Verificar desvio padr√£o\n",
    "                    std_val = non_null_data.std()\n",
    "                    if std_val < 0.0001:  # Praticamente zero\n",
    "                        print(f\"  ‚ö†Ô∏è  {col}: Desvio padr√£o muito baixo ({std_val:.6f})\")\n",
    "                        problematic_features.append(col)\n",
    "                    else:\n",
    "                        safe_features.append(col)\n",
    "                        print(f\"  ‚úÖ {col}: OK ({len(non_null_data)} valores, std={std_val:.2f})\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  {col}: Tipo n√£o num√©rico ({df[col].dtype})\")\n",
    "                problematic_features.append(col)\n",
    "        else:\n",
    "            print(f\"  ‚ùå {col}: Coluna n√£o encontrada no DataFrame\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADO DA VERIFICA√á√ÉO:\")\n",
    "    print(f\"  ‚Ä¢ Features seguras: {len(safe_features)}\")\n",
    "    print(f\"  ‚Ä¢ Features problem√°ticas: {len(problematic_features)}\")\n",
    "    \n",
    "    # Usar apenas features seguras\n",
    "    if len(safe_features) >= 2 and 'target_atraso' in safe_features:\n",
    "        try:\n",
    "            # Criar DataFrame apenas com features seguras\n",
    "            safe_df = df[safe_features].copy()\n",
    "            \n",
    "            # Preencher valores NaN com mediana para cada coluna\n",
    "            for col in safe_features:\n",
    "                if safe_df[col].isnull().any():\n",
    "                    median_val = safe_df[col].median()\n",
    "                    safe_df[col] = safe_df[col].fillna(median_val)\n",
    "                    print(f\"  üîß {col}: NaN preenchidos com mediana {median_val:.2f}\")\n",
    "            \n",
    "            # AGORA calcular correla√ß√£o com tratamento de erros\n",
    "            try:\n",
    "                corr_matrix = safe_df.corr(method='pearson')\n",
    "                \n",
    "                # Verificar se a matriz tem valores v√°lidos\n",
    "                if corr_matrix.isnull().any().any():\n",
    "                    print(\"  ‚ö†Ô∏è  Matriz de correla√ß√£o cont√©m valores NaN\")\n",
    "                    # Preencher NaN com 0\n",
    "                    corr_matrix = corr_matrix.fillna(0)\n",
    "                \n",
    "                # Garantir que temos a vari√°vel alvo\n",
    "                if 'target_atraso' in corr_matrix.columns:\n",
    "                    # Ordenar por correla√ß√£o com alvo\n",
    "                    target_corr = corr_matrix['target_atraso'].drop('target_atraso', errors='ignore')\n",
    "                    \n",
    "                    if len(target_corr) > 0:\n",
    "                        sorted_features = target_corr.abs().sort_values(ascending=False).index.tolist()\n",
    "                        sorted_features = ['target_atraso'] + sorted_features\n",
    "                        \n",
    "                        # Limitar a um n√∫mero razo√°vel para visualiza√ß√£o\n",
    "                        display_features = sorted_features[:min(10, len(sorted_features))]\n",
    "                        \n",
    "                        # Extrair submatriz\n",
    "                        display_matrix = corr_matrix.loc[display_features, display_features]\n",
    "                        \n",
    "                        # Plotar heatmap\n",
    "                        vmax = max(abs(display_matrix.min().min()), abs(display_matrix.max().max()))\n",
    "                        vmax = min(vmax, 1.0)  # Limitar entre -1 e 1\n",
    "                        \n",
    "                        im = ax2.imshow(display_matrix, cmap='RdBu_r', vmin=-vmax, vmax=vmax, aspect='auto')\n",
    "                        \n",
    "                        # Configurar ticks\n",
    "                        tick_positions = range(len(display_features))\n",
    "                        ax2.set_xticks(tick_positions)\n",
    "                        ax2.set_yticks(tick_positions)\n",
    "                        \n",
    "                        # Labels\n",
    "                        ax2.set_xticklabels([textwrap.fill(col, 12) for col in display_features], \n",
    "                                           rotation=45, ha='right', fontsize=8)\n",
    "                        ax2.set_yticklabels([textwrap.fill(col, 12) for col in display_features], \n",
    "                                           fontsize=8)\n",
    "                        \n",
    "                        # Adicionar valores\n",
    "                        for i in range(len(display_features)):\n",
    "                            for j in range(len(display_features)):\n",
    "                                value = display_matrix.iloc[i, j]\n",
    "                                if not pd.isna(value):\n",
    "                                    color = 'white' if abs(value) > 0.3 else 'black'\n",
    "                                    ax2.text(j, i, f'{value:.2f}', ha='center', va='center', \n",
    "                                            color=color, fontsize=7, fontweight='bold')\n",
    "                        \n",
    "                        plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "                        ax2.set_title(f'Correla√ß√£o ({len(safe_features)} features seguras)', fontsize=14, fontweight='bold')\n",
    "                        \n",
    "                    else:\n",
    "                        ax2.text(0.5, 0.5, 'Sem correla√ß√µes v√°lidas\\ncom vari√°vel alvo', \n",
    "                                 ha='center', va='center', fontsize=12)\n",
    "                else:\n",
    "                    ax2.text(0.5, 0.5, 'Vari√°vel alvo n√£o encontrada\\nna matriz de correla√ß√£o', \n",
    "                             ha='center', va='center', fontsize=12)\n",
    "                    \n",
    "            except Exception as calc_error:\n",
    "                ax2.text(0.5, 0.5, f'Erro c√°lculo correla√ß√£o:\\n{str(calc_error)[:30]}', \n",
    "                         ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        except Exception as df_error:\n",
    "            ax2.text(0.5, 0.5, f'Erro prepara√ß√£o dados:\\n{str(df_error)[:30]}', \n",
    "                     ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, f'Features seguras insuficientes\\n({len(safe_features)} de {len(top_corr_features)})', \n",
    "                 ha='center', va='center', fontsize=11)\n",
    "        print(f\"\\n‚ùå N√£o h√° features suficientes seguras para heatmap\")\n",
    "        \n",
    "else:\n",
    "    ax2.text(0.5, 0.5, f'Features correlacionadas\\ninsuficientes ({len(sorted_corr)})', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "\n",
    "# 3. Boxplot de features num√©ricas vs atraso\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "if len(sorted_corr) >= 3:\n",
    "    # Selecionar top 3 features num√©ricas\n",
    "    top_numeric_features = []\n",
    "    for col, _ in sorted_corr:\n",
    "        if col in filtered_numeric_cols and len(top_numeric_features) < 3:\n",
    "            top_numeric_features.append(col)\n",
    "    \n",
    "    if top_numeric_features:\n",
    "        # Preparar dados para boxplot\n",
    "        boxplot_data = []\n",
    "        boxplot_labels = []\n",
    "        \n",
    "        for feature in top_numeric_features:\n",
    "            # Amostrar para melhor visualiza√ß√£o\n",
    "            sample_size = min(1000, len(df))\n",
    "            df_sample = df.sample(sample_size, random_state=SEED)\n",
    "            \n",
    "            for target_value in [0, 1]:\n",
    "                subset = df_sample[df_sample['target_atraso'] == target_value][feature].dropna()\n",
    "                if len(subset) > 0:\n",
    "                    boxplot_data.append(subset.values)\n",
    "                    boxplot_labels.append(f\"{feature}\\n{'Pontual' if target_value == 0 else 'Atrasado'}\")\n",
    "        \n",
    "        # Plotar boxplot\n",
    "        if boxplot_data:\n",
    "            try:\n",
    "                bp = ax3.boxplot(boxplot_data, patch_artist=True, labels=boxplot_labels, \n",
    "                                showfliers=False)  # Ocultar outliers para clareza\n",
    "                \n",
    "                # Colorir boxes\n",
    "                colors = ['lightblue', 'lightcoral'] * len(top_numeric_features)\n",
    "                for patch, color in zip(bp['boxes'], colors):\n",
    "                    patch.set_facecolor(color)\n",
    "                \n",
    "                ax3.set_title('Boxplot: Features vs Atraso', fontsize=14, fontweight='bold')\n",
    "                ax3.set_ylabel('Valor da Feature')\n",
    "                ax3.tick_params(axis='x', rotation=45)\n",
    "            except:\n",
    "                ax3.text(0.5, 0.5, 'Erro ao gerar\\nboxplot', \n",
    "                         ha='center', va='center', fontsize=12)\n",
    "                ax3.set_title('Boxplot: Features vs Atraso', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Dados insuficientes\\npara boxplot', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "            ax3.set_title('Boxplot: Features vs Atraso', fontsize=14, fontweight='bold')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Dados insuficientes\\npara boxplot', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        ax3.set_title('Boxplot: Features vs Atraso', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Dados insuficientes\\npara boxplot', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax3.set_title('Boxplot: Features vs Atraso', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Taxa de atraso por companhia (top 10)\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "\n",
    "# Tentar identificar coluna de companhia a√©rea\n",
    "airline_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                for keyword in ['airline', 'carrier', 'companhia', 'op_carrier'])]\n",
    "\n",
    "if airline_cols:\n",
    "    airline_col = airline_cols[0]\n",
    "    \n",
    "    try:\n",
    "        # Calcular taxa de atraso por companhia\n",
    "        airline_delay_stats = df.groupby(airline_col).agg({\n",
    "            'target_atraso': ['mean', 'count']\n",
    "        }).round(3)\n",
    "        \n",
    "        airline_delay_stats.columns = ['taxa_atraso', 'total_voos']\n",
    "        airline_delay_stats = airline_delay_stats.sort_values('taxa_atraso', ascending=False)\n",
    "        \n",
    "        # Pegar top 10 companhias\n",
    "        top_airlines = airline_delay_stats.head(10)\n",
    "        \n",
    "        if len(top_airlines) > 0:\n",
    "            # Plotar\n",
    "            bars = ax4.bar(range(len(top_airlines)), top_airlines['taxa_atraso'] * 100,\n",
    "                          color=plt.cm.RdYlGn_r(np.linspace(0, 1, len(top_airlines))),\n",
    "                          edgecolor='black')\n",
    "            \n",
    "            ax4.set_title(f'Taxa de Atraso por {airline_col} (Top 10)', fontsize=14, fontweight='bold')\n",
    "            ax4.set_xlabel('Companhia A√©rea')\n",
    "            ax4.set_ylabel('Taxa de Atraso (%)')\n",
    "            \n",
    "            # Configurar labels do eixo X\n",
    "            x_labels = [str(idx) for idx in top_airlines.index]\n",
    "            ax4.set_xticks(range(len(x_labels)))\n",
    "            ax4.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "            \n",
    "            # Adicionar linhas de refer√™ncia\n",
    "            ax4.axhline(y=taxa_atraso, color='red', linestyle='--', linewidth=2, \n",
    "                        label=f'M√©dia Geral: {taxa_atraso:.1f}%')\n",
    "            ax4.axhline(y=50, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "            \n",
    "            # Adicionar valores nas barras\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                        f'{height:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "            \n",
    "            ax4.legend(fontsize=9)\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'Dados insuficientes\\npara an√°lise', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "            ax4.set_title('Taxa de Atraso por Companhia', fontsize=14, fontweight='bold')\n",
    "    except:\n",
    "        ax4.text(0.5, 0.5, 'Erro ao calcular\\ntaxas por companhia', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        ax4.set_title('Taxa de Atraso por Companhia', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Coluna de companhia\\nn√£o identificada', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax4.set_title('Taxa de Atraso por Companhia', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Distribui√ß√£o temporal (se houver coluna temporal)\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "if time_cols:\n",
    "    time_col = time_cols[0]\n",
    "    \n",
    "    # Tentar extrair hora do dia\n",
    "    try:\n",
    "        # Converter para datetime\n",
    "        df_time = df.copy()\n",
    "        df_time['datetime_temp'] = pd.to_datetime(df_time[time_col], errors='coerce')\n",
    "        \n",
    "        # Extrair hora\n",
    "        df_time['hora_extraida'] = df_time['datetime_temp'].dt.hour\n",
    "        \n",
    "        # Calcular taxa de atraso por hora\n",
    "        if 'hora_extraida' in df_time.columns:\n",
    "            hora_delay_stats = df_time.groupby('hora_extraida').agg({\n",
    "                'target_atraso': ['mean', 'count']\n",
    "            }).round(3)\n",
    "            \n",
    "            hora_delay_stats.columns = ['taxa_atraso', 'total_voos']\n",
    "            hora_delay_stats = hora_delay_stats.sort_index()\n",
    "            \n",
    "            if len(hora_delay_stats) > 0:\n",
    "                # Plotar\n",
    "                ax5.plot(hora_delay_stats.index, hora_delay_stats['taxa_atraso'] * 100,\n",
    "                        marker='o', color='darkred', linewidth=2)\n",
    "                ax5.fill_between(hora_delay_stats.index, 0, hora_delay_stats['taxa_atraso'] * 100,\n",
    "                                alpha=0.3, color='lightcoral')\n",
    "                \n",
    "                ax5.set_title('Taxa de Atraso por Hora do Dia', fontsize=14, fontweight='bold')\n",
    "                ax5.set_xlabel('Hora do Dia')\n",
    "                ax5.set_ylabel('Taxa de Atraso (%)')\n",
    "                ax5.set_xticks(range(0, 24, 2))\n",
    "                ax5.grid(True, alpha=0.3)\n",
    "                ax5.axhline(y=taxa_atraso, color='blue', linestyle='--', \n",
    "                           label=f'M√©dia: {taxa_atraso:.1f}%')\n",
    "                ax5.legend()\n",
    "            else:\n",
    "                ax5.text(0.5, 0.5, 'N√£o foi poss√≠vel\\nanalisar horas', \n",
    "                         ha='center', va='center', fontsize=12)\n",
    "                ax5.set_title('Distribui√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            ax5.text(0.5, 0.5, 'N√£o foi poss√≠vel\\nextrair horas', \n",
    "                     ha='center', va='center', fontsize=12)\n",
    "            ax5.set_title('Distribui√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "    except:\n",
    "        ax5.text(0.5, 0.5, 'Erro ao processar\\ndados temporais', \n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        ax5.set_title('Distribui√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, 'Sem dados temporais\\npara an√°lise', \n",
    "             ha='center', va='center', fontsize=12)\n",
    "    ax5.set_title('Distribui√ß√£o Temporal', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 6. Resumo estat√≠stico\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "\n",
    "# Corrigir a formata√ß√£o do f-string\n",
    "if sorted_corr:\n",
    "    top_corr_col = sorted_corr[0][0]\n",
    "    top_corr_val = sorted_corr[0][1]\n",
    "    top_corr_text = f\"{top_corr_col}\"\n",
    "    top_corr_val_text = f\"{top_corr_val:.3f}\"\n",
    "else:\n",
    "    top_corr_text = \"N/A\"\n",
    "    top_corr_val_text = \"N/A\"\n",
    "\n",
    "# Criar texto de resumo\n",
    "resumo_text = f\"\"\"\n",
    "üìä RESUMO DA AN√ÅLISE ESTRAT√âGICA\n",
    "\n",
    "üìà DADOS GERAIS:\n",
    "‚Ä¢ Total de voos: {total_flights:,}\n",
    "‚Ä¢ Features: {df.shape[1]}\n",
    "‚Ä¢ Taxa de atraso: {taxa_atraso:.1f}%\n",
    "‚Ä¢ Balanceamento: {balance_ratio:.3f}\n",
    "\n",
    "üéØ VARI√ÅVEL ALVO:\n",
    "‚Ä¢ Coluna origem: {target_col}\n",
    "‚Ä¢ Limite: ‚â• {LIMITE_ATRASO} min\n",
    "‚Ä¢ Pontual: {target_stats.get(0, 0):,}\n",
    "‚Ä¢ Atrasado: {target_stats.get(1, 0):,}\n",
    "\n",
    "üîç FEATURES DESTAQUE:\n",
    "‚Ä¢ Top correlacionada: {top_corr_text}\n",
    "‚Ä¢ Correla√ß√£o: {top_corr_val_text}\n",
    "\n",
    "‚ö†Ô∏è PROBLEMAS IDENTIFICADOS:\n",
    "‚Ä¢ Colunas com vazamento: {len(leakage_columns)}\n",
    "‚Ä¢ >50% missing: {len(high_missing_cols)}\n",
    "‚Ä¢ Outliers extremos: {len(outlier_analysis)}\n",
    "\n",
    "üìã PR√ìXIMOS PASSOS:\n",
    "1. Remover features problem√°ticas\n",
    "2. Tratar missing values\n",
    "3. Codificar vari√°veis categ√≥ricas\n",
    "4. Balancear dataset (se necess√°rio)\n",
    "\"\"\"\n",
    "\n",
    "ax6.text(0.05, 0.95, resumo_text, transform=ax6.transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SALVAR RELAT√ìRIO COMPLETO\n",
    "# ============================================================================\n",
    "print(\"\\nüíæ SALVANDO RELAT√ìRIO COMPLETO...\")\n",
    "\n",
    "# Corrigir a formata√ß√£o para o relat√≥rio tamb√©m\n",
    "if sorted_corr:\n",
    "    relatorio_corr_col = sorted_corr[0][0]\n",
    "    relatorio_corr_val = f\"{sorted_corr[0][1]:.3f}\"\n",
    "else:\n",
    "    relatorio_corr_col = \"N/A\"\n",
    "    relatorio_corr_val = \"N/A\"\n",
    "\n",
    "# Criar relat√≥rio em formato notebook-friendly\n",
    "relatorio = f\"\"\"# üìä RELAT√ìRIO DE AN√ÅLISE ESTRAT√âGICA\n",
    "## Dataset: {DATA_PATH}\n",
    "## Data da an√°lise: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## 1. RESUMO EXECUTIVO\n",
    "- **Total de voos analisados**: {total_flights:,}\n",
    "- **Taxa de atrasos**: {taxa_atraso:.1f}%\n",
    "- **Features dispon√≠veis**: {df.shape[1]}\n",
    "- **Problemas cr√≠ticos identificados**: {len(limitations)}\n",
    "\n",
    "## 2. VARI√ÅVEL ALVO\n",
    "- **Coluna origem**: {target_col}\n",
    "- **Limite de atraso**: ‚â• {LIMITE_ATRASO} minutos\n",
    "- **Distribui√ß√£o**: \n",
    "  - Pontual (0): {target_stats.get(0, 0):,} voos ({(target_stats.get(0, 0)/total_flights*100):.1f}%)\n",
    "  - Atrasado (1): {target_stats.get(1, 0):,} voos ({taxa_atraso:.1f}%)\n",
    "- **Balanceamento**: {balance_ratio:.3f} ({'DESBALANCEADO' if balance_ratio < 0.5 else 'BALANCEADO'})\n",
    "\n",
    "## 3. FEATURES PROMISSORAS (Top 10)\n",
    "{chr(10).join([f\"{i+1}. {feature}\" for i, feature in enumerate(potential_features[:10])])}\n",
    "\n",
    "## 4. PROBLEMAS IDENTIFICADOS\n",
    "{chr(10).join([f\"- {limitation}\" for limitation in limitations])}\n",
    "\n",
    "## 5. RECOMENDA√á√ïES\n",
    "1. **Remover features com vazamento**: {len(leakage_columns)} colunas identificadas\n",
    "2. **Tratar missing values**: {len(high_missing_cols)} colunas com >50% ausentes\n",
    "3. **Balancear dataset**: {'Necess√°rio' if balance_ratio < 0.5 else 'N√£o necess√°rio'}\n",
    "4. **Codificar vari√°veis categ√≥ricas**: {len(meaningful_categorical)} colunas identificadas\n",
    "5. **Tratar outliers**: {'Necess√°rio' if outlier_analysis else 'N√£o cr√≠tico'}\n",
    "\n",
    "## 6. BASELINE PARA MODELAGEM\n",
    "- **Acur√°cia do baseline**: {baseline_accuracy:.1f}%\n",
    "- **Recall baseline (atrasos)**: {100 if majority_class_label == 1 else 0:.1f}%\n",
    "- **Precision baseline (atrasos)**: {taxa_atraso if majority_class_label == 1 else 0:.1f}%\n",
    "\n",
    "## 7. FEATURE MAIS CORRELACIONADA\n",
    "- **Feature**: {relatorio_corr_col}\n",
    "- **Correla√ß√£o com atraso**: {relatorio_corr_val}\n",
    "\"\"\"\n",
    "\n",
    "# Salvar relat√≥rio\n",
    "relatorio_path = 'datascience/1_understanding/docs/business_insights.md'\n",
    "with open(relatorio_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(relatorio)\n",
    "\n",
    "print(f\"‚úÖ Relat√≥rio salvo em '{relatorio_path}'\")\n",
    "\n",
    "# Salvar notebook com a an√°lise\n",
    "notebook_content = f\"\"\"# üìä AN√ÅLISE ESTRAT√âGICA DO DATASET DE VOOS\n",
    "## Story 1.1: An√°lise Explorat√≥ria Estrat√©gica\n",
    "\n",
    "### Configura√ß√µes Iniciais\n",
    "- **Seed**: {SEED}\n",
    "- **Dataset**: {DATA_PATH}\n",
    "- **Data da an√°lise**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "### An√°lise Realizada\n",
    "1. **Diagn√≥stico ultrarr√°pido**: Carregamento otimizado em <5min\n",
    "2. **An√°lise da vari√°vel alvo**: Taxa de atraso de {taxa_atraso:.1f}%\n",
    "3. **Identifica√ß√£o de features**: {len(potential_features)} features potenciais\n",
    "4. **Detec√ß√£o de problemas**: {len(limitations)} problemas cr√≠ticos\n",
    "5. **Visualiza√ß√µes estrat√©gicas**: Dashboard completo gerado\n",
    "\n",
    "### Arquivos Gerados\n",
    "1. `target_variable_analysis.csv`: An√°lise detalhada da vari√°vel alvo\n",
    "2. `business_insights.md`: Relat√≥rio executivo com insights\n",
    "3. Dashboard visual completo (exibido acima)\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "1. Executar Story 1.2: An√°lise Univariada\n",
    "2. Remover features problem√°ticas\n",
    "3. Preparar dados para modelagem\n",
    "\n",
    "### Features Mais Promissoras\n",
    "{chr(10).join([f\"{i+1}. {feature}\" for i, feature in enumerate(potential_features[:10])])}\n",
    "\"\"\"\n",
    "\n",
    "# Salvar como notebook\n",
    "notebook_path = 'datascience/1_understanding/data/quick_analysis_report.ipynb'\n",
    "# Nota: Em ambiente real, salvar√≠amos como .ipynb\n",
    "# Por simplicidade, salvaremos como .txt para demonstra√ß√£o\n",
    "txt_path = notebook_path.replace('.ipynb', '_summary.txt')\n",
    "with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(notebook_content)\n",
    "\n",
    "print(f\"‚úÖ Sum√°rio da an√°lise salvo em '{txt_path}'\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONCLUS√ÉO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ STORY 1.1 COMPLETADA COM SUCESSO!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüì¶ ENTREG√ÅVEIS GERADOS:\")\n",
    "print(\"1. üìä An√°lise completa do dataset\")\n",
    "print(\"2. üìà Distribui√ß√£o da vari√°vel alvo\")\n",
    "print(\"3. üéØ Lista de 10 features promissoras\")\n",
    "print(\"4. ‚ö†Ô∏è  Identifica√ß√£o de problemas cr√≠ticos\")\n",
    "print(\"5. üìä Dashboard de visualiza√ß√µes estrat√©gicas\")\n",
    "print(\"6. üìÑ Relat√≥rio executivo (business_insights.md)\")\n",
    "print(\"7. üìã An√°lise da vari√°vel alvo (target_variable_analysis.csv)\")\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASSOS RECOMENDADOS:\")\n",
    "print(\"1. Remover colunas com >50% missing values\")\n",
    "print(\"2. Eliminar features com vazamento de dados\")\n",
    "print(\"3. Iniciar Story 1.2: An√°lise Univariada\")\n",
    "print(\"4. Preparar pipeline de pr√©-processamento\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa9517",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# üì¶ SALVAR ENTREG√ÅVEIS COMPLETOS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ SALVANDO ENTREG√ÅVEIS COMPLETOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Criar estrutura de pastas\n",
    "print(\"\\nüìÅ 1. CRIANDO ESTRUTURA DE PASTAS...\")\n",
    "folders_to_create = [\n",
    "    'datascience/1_understanding/notebooks',\n",
    "    'datascience/1_understanding/data',\n",
    "    'datascience/1_understanding/docs',\n",
    "    'datascience/1_understanding/reports',\n",
    "    'datascience/1_understanding/visualizations'\n",
    "]\n",
    "\n",
    "for folder in folders_to_create:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"   ‚úÖ {folder}\")\n",
    "\n",
    "# 2. Salvar amostra dos dados originais\n",
    "print(\"\\nüíæ 2. SALVANDO AMOSTRA DOS DADOS...\")\n",
    "try:\n",
    "    # Salvar dataset completo (ou amostra se for muito grande)\n",
    "    if len(df) > 100000:\n",
    "        # Salvar amostra de 10% para compartilhamento\n",
    "        df_sample = df.sample(frac=0.1, random_state=SEED)\n",
    "        sample_path = 'datascience/1_understanding/data/flight_data_sample_10pct.csv'\n",
    "        df_sample.to_csv(sample_path, index=False)\n",
    "        print(f\"   ‚úÖ Dataset muito grande, salvando amostra de 10%: {len(df_sample):,} linhas\")\n",
    "    else:\n",
    "        # Salvar dataset completo\n",
    "        original_path = 'datascience/1_understanding/data/flight_data_original.csv'\n",
    "        df.to_csv(original_path, index=False)\n",
    "        print(f\"   ‚úÖ Dataset original salvo: {len(df):,} linhas\")\n",
    "    \n",
    "    # Salvar tamb√©m o dataset com a vari√°vel alvo\n",
    "    processed_path = 'datascience/1_understanding/data/flight_data_with_target.csv'\n",
    "    df.to_csv(processed_path, index=False)\n",
    "    print(f\"   ‚úÖ Dataset com vari√°vel alvo salvo\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Erro ao salvar dados: {e}\")\n",
    "\n",
    "# 3. Exportar c√≥digo atual como notebook Jupyter (.ipynb)\n",
    "print(\"\\nüìì 3. EXPORTANDO COMO NOTEBOOK JUPYTER...\")\n",
    "try:\n",
    "    # Criar conte√∫do do notebook\n",
    "    notebook_content = {\n",
    "        \"cells\": [\n",
    "            {\n",
    "                \"cell_type\": \"markdown\",\n",
    "                \"metadata\": {},\n",
    "                \"source\": [\n",
    "                    \"# üìä Story 1.1: An√°lise Explorat√≥ria Estrat√©gica\\n\",\n",
    "                    \"## üìã Vis√£o Geral\\n\",\n",
    "                    f\"**Data de execu√ß√£o:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\",\n",
    "                    f\"**Dataset:** {DATA_PATH}\\n\",\n",
    "                    f\"**Total de registros:** {len(df):,}\\n\",\n",
    "                    f\"**Taxa de atrasos:** {taxa_atraso:.2f}%\\n\",\n",
    "                    \"\\n## üéØ Objetivos\\n\",\n",
    "                    \"1. Diagn√≥stico ultrarr√°pido do dataset\\n\",\n",
    "                    \"2. An√°lise da vari√°vel alvo\\n\", \n",
    "                    \"3. Identifica√ß√£o de features promissoras\\n\",\n",
    "                    \"4. Detec√ß√£o de problemas cr√≠ticos\\n\",\n",
    "                    \"5. Visualiza√ß√µes estrat√©gicas\\n\",\n",
    "                    \"\\n## üìä M√©tricas Principais\\n\",\n",
    "                    f\"- Acur√°cia baseline: {baseline_accuracy:.2f}%\\n\",\n",
    "                    f\"- Balanceamento: {balance_ratio:.3f}\\n\",\n",
    "                    f\"- Features problem√°ticas: {len(limitations)}\\n\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"cell_type\": \"code\",\n",
    "                \"execution_count\": None,\n",
    "                \"metadata\": {},\n",
    "                \"outputs\": [],\n",
    "                \"source\": [\n",
    "                    \"# Importa√ß√µes b√°sicas\\n\",\n",
    "                    \"import pandas as pd\\n\",\n",
    "                    \"import numpy as np\\n\",\n",
    "                    \"import matplotlib.pyplot as plt\\n\",\n",
    "                    \"import seaborn as sns\\n\",\n",
    "                    \"import os\\n\",\n",
    "                    \"from datetime import datetime\\n\",\n",
    "                    \"\\n\",\n",
    "                    \"# Configura√ß√µes\\n\",\n",
    "                    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "                    \"plt.style.use('seaborn-v0_8-darkgrid')\\n\",\n",
    "                    \"sns.set_palette('husl')\\n\",\n",
    "                    \"\\n\",\n",
    "                    \"SEED = 42\\n\",\n",
    "                    \"np.random.seed(SEED)\\n\",\n",
    "                    \"print('‚úÖ Ambiente configurado')\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"cell_type\": \"markdown\",\n",
    "                \"metadata\": {},\n",
    "                \"source\": [\n",
    "                    \"## üìà Resumo da An√°lise\\n\",\n",
    "                    \"\\n\",\n",
    "                    \"### üìä Dados Gerais\\n\",\n",
    "                    f\"- Total de voos: {total_flights:,}\\n\",\n",
    "                    f\"- Features dispon√≠veis: {df.shape[1]}\\n\",\n",
    "                    f\"- Taxa de atrasos: {taxa_atraso:.2f}%\\n\",\n",
    "                    f\"- Balanceamento: {balance_ratio:.3f}\\n\",\n",
    "                    \"\\n\",\n",
    "                    \"### üéØ Vari√°vel Alvo\\n\",\n",
    "                    f\"- Coluna origem: {target_col}\\n\",\n",
    "                    f\"- Limite: ‚â• {LIMITE_ATRASO} minutos\\n\",\n",
    "                    f\"- Pontual: {target_stats.get(0, 0):,} voos\\n\",\n",
    "                    f\"- Atrasado: {target_stats.get(1, 0):,} voos\\n\",\n",
    "                    \"\\n\",\n",
    "                    \"### üîç Top 5 Features Promissoras\\n\"\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"metadata\": {\n",
    "            \"kernelspec\": {\n",
    "                \"display_name\": \"Python 3\",\n",
    "                \"language\": \"python\",\n",
    "                \"name\": \"python3\"\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"python\",\n",
    "                \"version\": \"3.8.0\"\n",
    "            }\n",
    "        },\n",
    "        \"nbformat\": 4,\n",
    "        \"nbformat_minor\": 4\n",
    "    }\n",
    "    \n",
    "    # Adicionar c√©lulas para cada feature promissora\n",
    "    for i, feature in enumerate(potential_features[:5], 1):\n",
    "        notebook_content[\"cells\"][2][\"source\"].append(f\"{i}. {feature}\\n\")\n",
    "    \n",
    "    # Salvar como arquivo .py tamb√©m (backup)\n",
    "    py_path = 'datascience/1_understanding/notebooks/story_1_1_analise_estrategica.py'\n",
    "    \n",
    "    # Ler o c√≥digo atual (simplificado)\n",
    "    with open(__file__, 'r', encoding='utf-8') as f:\n",
    "        current_code = f.read()\n",
    "    \n",
    "    # Salvar vers√£o .py\n",
    "    with open(py_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(current_code)\n",
    "    \n",
    "    print(f\"   ‚úÖ C√≥digo salvo como: {py_path}\")\n",
    "    \n",
    "    # Criar um notebook simplificado em .ipynb (formato texto)\n",
    "    import json\n",
    "    \n",
    "    ipynb_path = 'datascience/1_understanding/notebooks/story_1_1_analise_estrategica.ipynb'\n",
    "    \n",
    "    # Adicionar mais c√©lulas com resultados importantes\n",
    "    results_cell = {\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"## üìä Resultados da An√°lise\\n\",\n",
    "            \"\\n\",\n",
    "            \"### üîù Top 10 Features por Correla√ß√£o\\n\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if sorted_corr:\n",
    "        for i, (col, corr) in enumerate(sorted_corr[:10], 1):\n",
    "            results_cell[\"source\"].append(f\"{i}. **{col}**: {corr:.3f}\\n\")\n",
    "    \n",
    "    results_cell[\"source\"].extend([\n",
    "        \"\\n### ‚ö†Ô∏è Problemas Identificados\\n\"\n",
    "    ])\n",
    "    \n",
    "    for limitation in limitations[:5]:\n",
    "        results_cell[\"source\"].append(f\"- {limitation}\\n\")\n",
    "    \n",
    "    notebook_content[\"cells\"].append(results_cell)\n",
    "    \n",
    "    # Adicionar c√©lula com visualiza√ß√µes salvas\n",
    "    viz_cell = {\n",
    "        \"cell_type\": \"markdown\",\n",
    "        \"metadata\": {},\n",
    "        \"source\": [\n",
    "            \"## üìà Visualiza√ß√µes Geradas\\n\",\n",
    "            \"\\n\",\n",
    "            \"As seguintes visualiza√ß√µes foram geradas e salvas:\\n\",\n",
    "            \"\\n\",\n",
    "            \"1. **Dashboard Estrat√©gico** (6 gr√°ficos combinados)\\n\",\n",
    "            \"2. **Distribui√ß√£o da Vari√°vel Alvo**\\n\",\n",
    "            \"3. **Taxa de Atraso por Companhia**\\n\",\n",
    "            \"4. **Boxplot: Features vs Atraso**\\n\",\n",
    "            \"\\n\",\n",
    "            \"### üìÅ Arquivos Salvos\\n\",\n",
    "            f\"- `target_variable_analysis.csv` - An√°lise da vari√°vel alvo\\n\",\n",
    "            f\"- `business_insights.md` - Relat√≥rio executivo\\n\",\n",
    "            f\"- `quick_analysis_report.txt` - Resumo da an√°lise\\n\",\n",
    "            f\"- `flight_data_with_target.csv` - Dataset processado\\n\"\n",
    "        ]\n",
    "    }\n",
    "    notebook_content[\"cells\"].append(viz_cell)\n",
    "    \n",
    "    # Salvar notebook .ipynb\n",
    "    with open(ipynb_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(notebook_content, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Notebook Jupyter salvo como: {ipynb_path}\")\n",
    "    \n",
    "    # Criar tamb√©m um notebook HTML para visualiza√ß√£o f√°cil\n",
    "    try:\n",
    "        import nbformat\n",
    "        from nbconvert import HTMLExporter\n",
    "        \n",
    "        nb = nbformat.reads(json.dumps(notebook_content), as_version=4)\n",
    "        html_exporter = HTMLExporter()\n",
    "        html_data, _ = html_exporter.from_notebook_node(nb)\n",
    "        \n",
    "        html_path = 'datascience/1_understanding/notebooks/story_1_1_analise_estrategica.html'\n",
    "        with open(html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_data)\n",
    "        \n",
    "        print(f\"   ‚úÖ Vers√£o HTML salva como: {html_path}\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  N√£o foi poss√≠vel gerar vers√£o HTML (nbconvert n√£o dispon√≠vel)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Erro ao exportar notebook: {e}\")\n",
    "    print(\"   ‚ÑπÔ∏è  Salvando apenas vers√£o .py\")\n",
    "\n",
    "# 4. Salvar screenshots das visualiza√ß√µes\n",
    "print(\"\\nüñºÔ∏è  4. SALVANDO VISUALIZA√á√ïES...\")\n",
    "try:\n",
    "    # Salvar a figura atual (dashboard)\n",
    "    dashboard_path = 'datascience/1_understanding/visualizations/dashboard_estrategico.png'\n",
    "    fig.savefig(dashboard_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"   ‚úÖ Dashboard salvo como: {dashboard_path}\")\n",
    "    \n",
    "    # Salvar gr√°ficos individuais\n",
    "    # 1. Distribui√ß√£o vari√°vel alvo\n",
    "    fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    ax1.pie(contagem_alvo, labels=labels_alvo, autopct='%1.1f%%', \n",
    "            colors=cores_alvo, explode=(0, 0.05))\n",
    "    ax1.set_title('Distribui√ß√£o da Vari√°vel Alvo', fontsize=14, fontweight='bold')\n",
    "    fig1.savefig('datascience/1_understanding/visualizations/distribuicao_alvo.png', \n",
    "                 dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "    \n",
    "    # 2. Taxa por companhia (se dispon√≠vel)\n",
    "    if airline_cols:\n",
    "        fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "        # ... c√≥digo do gr√°fico de companhia ...\n",
    "        fig2.savefig('datascience/1_understanding/visualizations/taxa_por_companhia.png', \n",
    "                     dpi=150, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "    \n",
    "    print(\"   ‚úÖ Visualiza√ß√µes individuais salvas\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Erro ao salvar visualiza√ß√µes: {e}\")\n",
    "\n",
    "# 5. Criar README para documenta√ß√£o\n",
    "print(\"\\nüìù 5. CRIANDO DOCUMENTA√á√ÉO...\")\n",
    "readme_content = f\"\"\"# üìä Story 1.1: An√°lise Explorat√≥ria Estrat√©gica\n",
    "\n",
    "## üìã Sobre\n",
    "An√°lise explorat√≥ria inicial do dataset de voos para compreens√£o dos dados e identifica√ß√£o de problemas cr√≠ticos.\n",
    "\n",
    "## üóìÔ∏è Data de Execu√ß√£o\n",
    "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## üìà M√©tricas Principais\n",
    "- **Total de voos**: {total_flights:,}\n",
    "- **Taxa de atrasos**: {taxa_atraso:.2f}%\n",
    "- **Balanceamento**: {balance_ratio:.3f}\n",
    "- **Acur√°cia baseline**: {baseline_accuracy:.2f}%\n",
    "\n",
    "## üìÅ Estrutura de Arquivos\n",
    "\n",
    "### üìì Notebooks\n",
    "- `story_1_1_analise_estrategica.ipynb` - Notebook Jupyter completo\n",
    "- `story_1_1_analise_estrategica.py` - C√≥digo Python\n",
    "- `story_1_1_analise_estrategica.html` - Vers√£o HTML (se dispon√≠vel)\n",
    "\n",
    "### üìä Dados\n",
    "- `flight_data_with_target.csv` - Dataset com vari√°vel alvo\n",
    "- `target_variable_analysis.csv` - An√°lise da vari√°vel alvo\n",
    "- `quick_analysis_report.txt` - Resumo da an√°lise\n",
    "\n",
    "### üìÑ Documenta√ß√£o\n",
    "- `business_insights.md` - Insights de neg√≥cio\n",
    "- `visualizations/` - Gr√°ficos e dashboards\n",
    "\n",
    "### üîç Features Promissoras (Top 5)\n",
    "{chr(10).join([f\"1. {feature}\" for i, feature in enumerate(potential_features[:5], 1)])}\n",
    "\n",
    "## ‚ö†Ô∏è Problemas Identificados\n",
    "{chr(10).join([f\"- {limitation}\" for limitation in limitations[:3]])}\n",
    "\n",
    "## üöÄ Pr√≥ximos Passos\n",
    "1. Executar Story 1.2: An√°lise Univariada\n",
    "2. Tratar valores missing identificados\n",
    "3. Remover features com vazamento de dados\n",
    "4. Balancear dataset se necess√°rio\n",
    "\n",
    "## üë§ Respons√°vel\n",
    "@ananda.matos\n",
    "\n",
    "## üìä Status\n",
    "‚úÖ COMPLETADA - {datetime.now().strftime('%d/%m/%Y')}\n",
    "\"\"\"\n",
    "\n",
    "readme_path = 'datascience/1_understanding/README.md'\n",
    "with open(readme_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"   ‚úÖ README criado: {readme_path}\")\n",
    "\n",
    "# 6. Criar arquivo de configura√ß√£o\n",
    "print(\"\\n‚öôÔ∏è  6. CRIANDO ARQUIVO DE CONFIGURA√á√ÉO...\")\n",
    "config_content = f\"\"\"# Configura√ß√µes da Story 1.1\n",
    "\n",
    "[PROJECT]\n",
    "name = \"An√°lise Explorat√≥ria Estrat√©gica\"\n",
    "version = \"1.0.0\"\n",
    "author = \"@ananda.matos\"\n",
    "date = \"{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "[DATASET]\n",
    "original_file = \"{DATA_PATH}\"\n",
    "rows = {len(df)}\n",
    "columns = {df.shape[1]}\n",
    "target_column = \"{target_col}\"\n",
    "delay_threshold = {LIMITE_ATRASO}\n",
    "\n",
    "[ANALYSIS]\n",
    "delay_rate = {taxa_atraso:.2f}\n",
    "balance_ratio = {balance_ratio:.3f}\n",
    "baseline_accuracy = {baseline_accuracy:.2f}\n",
    "problematic_features = {len(limitations)}\n",
    "\n",
    "[PATHS]\n",
    "notebooks = \"datascience/1_understanding/notebooks/\"\n",
    "data = \"datascience/1_understanding/data/\"\n",
    "docs = \"datascience/1_understanding/docs/\"\n",
    "visualizations = \"datascience/1_understanding/visualizations/\"\n",
    "\"\"\"\n",
    "\n",
    "config_path = 'datascience/1_understanding/project_config.ini'\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"   ‚úÖ Configura√ß√£o salva: {config_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# üìã RESUMO FINAL DOS ENTREG√ÅVEIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ RESUMO DOS ENTREG√ÅVEIS SALVOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìì NOTEBOOKS E C√ìDIGO:\n",
    "   ‚Ä¢ story_1_1_analise_estrategica.ipynb  (Jupyter Notebook)\n",
    "   ‚Ä¢ story_1_1_analise_estrategica.py     (C√≥digo Python)\n",
    "   ‚Ä¢ story_1_1_analise_estrategica.html   (HTML - se gerado)\n",
    "\n",
    "üìä DADOS E AN√ÅLISES:\n",
    "   ‚Ä¢ flight_data_with_target.csv          (Dataset processado)\n",
    "   ‚Ä¢ target_variable_analysis.csv         (An√°lise vari√°vel alvo)\n",
    "   ‚Ä¢ quick_analysis_report.txt            (Resumo da an√°lise)\n",
    "\n",
    "üìÑ DOCUMENTA√á√ÉO:\n",
    "   ‚Ä¢ business_insights.md                 (Insights de neg√≥cio)\n",
    "   ‚Ä¢ README.md                            (Documenta√ß√£o do projeto)\n",
    "   ‚Ä¢ project_config.ini                   (Configura√ß√µes)\n",
    "\n",
    "üñºÔ∏è  VISUALIZA√á√ïES:\n",
    "   ‚Ä¢ dashboard_estrategico.png            (Dashboard completo)\n",
    "   ‚Ä¢ distribuicao_alvo.png                (Distribui√ß√£o vari√°vel alvo)\n",
    "   ‚Ä¢ taxa_por_companhia.png               (Taxa por companhia)\n",
    "\n",
    "üìÅ ESTRUTURA CRIADA:\n",
    "   ‚Ä¢ datascience/1_understanding/notebooks/\n",
    "   ‚Ä¢ datascience/1_understanding/data/\n",
    "   ‚Ä¢ datascience/1_understanding/docs/\n",
    "   ‚Ä¢ datascience/1_understanding/visualizations/\n",
    "   ‚Ä¢ datascience/1_understanding/reports/\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ TODOS OS ENTREG√ÅVEIS FORAM SALVOS COM SUCESSO!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6cdc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
